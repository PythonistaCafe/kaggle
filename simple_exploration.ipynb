{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "In this exploration notebook, we shall try to uncover the basic information about the dataset which will help us build our models / features.\n\nLet us start with importing the necessary modules.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.set_option('display.max_columns', 500)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "First let us import the train file and get some idea about the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.shape",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are quite a few variables in this dataset. \n\nLet us start with target variable exploration - 'price_doc'. First let us do a scatter plot to see if there are any outliers in the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.price_doc.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Looks okay to me. Also since the metric is RMSLE, I think it is okay to have it as such. However if needed, one can truncate the high values. \n\nWe can now bin the 'price_doc' and plot it.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.distplot(train_df.price_doc.values, bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Certainly a very long right tail. Since our metric is Root Mean Square **Logarithmic** error, let us plot the log of price_doc variable.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.distplot(np.log(train_df.price_doc.values), bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This looks much better than the previous one. \n\nNow let us see how the median housing price change with time. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df['yearmonth'] = train_df['timestamp'].apply(lambda x: x[:4]+x[5:7])\ngrouped_df = train_df.groupby('yearmonth')['price_doc'].aggregate(np.median).reset_index()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.barplot(grouped_df.yearmonth.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Year Month', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are some variations in the median price with respect to time. Towards the end, there seems to be some linear increase in the price values.\n\nNow let us dive into other variables and see. Let us first start with getting the count of different data types. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So majority of them are numerical variables with 15 factor variables and 1 date variable.\n\nLet us explore the number of missing values in each column.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count']>0]\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='y')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Seems variables are found to missing as groups.\n\nSince there are 292 variables, let us build a basic xgboost model and then explore only the important variables.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for f in train_df.columns:\n    if train_df[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df.price_doc.values\ntrain_X = train_df.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So the top 5 variables and their description from the data dictionary are:\n\n 1. full_sq - total area in square meters, including loggias, balconies and other non-residential areas\n 2. life_sq - living area in square meters, excluding loggias, balconies and other non-residential areas\n 3. floor - for apartments, floor of the building\n 4. max_floor - number of floors in the building\n 5. build_year - year built\n\nNow let us see how these important variables are distributed with respect to target variable.\n\n**Total area in square meters:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ulimit = np.percentile(train_df.price_doc.values, 99.5)\nllimit = np.percentile(train_df.price_doc.values, 0.5)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\ncol = \"full_sq\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=np.log1p(train_df.full_sq.values), y=np.log1p(train_df.price_doc.values), size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Log of Total area in square metre', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Living area in square meters:**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "col = \"life_sq\"\ntrain_df[col].fillna(0, inplace=True)\nulimit = np.percentile(train_df[col].values, 95)\nllimit = np.percentile(train_df[col].values, 5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=np.log1p(train_df.life_sq.values), y=np.log1p(train_df.price_doc.values), \n              kind='kde', size=10)\nplt.ylabel('Log of Price', fontsize=12)\nplt.xlabel('Log of living area in square metre', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Floor:**\n\nWe will see the count plot of floor variable.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.countplot(x=\"floor\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The distribution is right skewed. There are some good drops in between (5 to 6, 9 to 10, 12 to 13, 17 to 18). Now let us see how the price changes with respect to floors.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "grouped_df = train_df.groupby('floor')['price_doc'].aggregate(np.median).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df.floor.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This shows an overall increasing trend (individual houses seems to be costlier as well - check price of 0 floor houses). \nA sudden increase in the house price is also observed at floor 18.\n\n**Max floor:**\n\nTotal number of floors in the building is one another important variable. So let us plot that one and see.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.countplot(x=\"max_floor\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Max floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We could see that there are few tall bars in between (at 5,9,12,17 - similar to drop in floors in the previous graph). May be there are some norms / restrictions on the number of maximum floors present(?). \n\nNow let us see how the median prices vary with the max floors. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8))\nsns.boxplot(x=\"max_floor\", y=\"price_doc\", data=train_df)\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Max Floor number', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "More to come. Stay tuned.!",
      "metadata": {}
    }
  ]
}